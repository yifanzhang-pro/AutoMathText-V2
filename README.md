# ğŸš€ AutoMathText-V2: A 2.46 Trillion Token AI-Curated STEM Pretraining Dataset

[![arXiv](https://img.shields.io/badge/arXiv-2402.07625-b31b1b.svg)](https://arxiv.org/abs/2402.07625)
[![Website](https://img.shields.io/badge/Project-Website-green)](https://iiis-ai.github.io/AutoMathText-V2)Â 
[![Technical Report](https://img.shields.io/badge/Technical-Report-blue)](https://iiis-ai.github.io/AutoMathText-V2/AutoMathText-V2.pdf)
[![License: CC-BY-SA-4.0](https://img.shields.io/badge/License-AutoMathText-yellow.svg)](https://github.com/iiis-ai/AutoMathText-V2/blob/master/LICENSE)
[![AutoMathText-V2](https://img.shields.io/badge/Huggingface-Datasets-blue)](https://huggingface.co/datasets/OpenSQZ/AutoMathText-V2) 

ğŸ“Š **AutoMathText-V2** consists of **2.46 trillion tokens** of high-quality, deduplicated text spanning web content, mathematics, code, reasoning, and bilingual data. This dataset was meticulously curated using a **three-tier deduplication pipeline** and **AI-powered quality assessment** to provide superior training data for large language models.

Our dataset combines **50+ premium data sources** with advanced processing techniques including **semantic deduplication**, **contamination detection**, and **intelligent text cleaning** to deliver exceptional model performance across diverse domains.

## ğŸ¯ What makes AutoMathText-V2 special?

- **ğŸ”¢ STEM Concentration**: Specially optimized for STEM content (especially Math) 
- **ğŸ” Triple Deduplication**: Exact â†’ Fuzzy (MinHash+LSH) â†’ Semantic (GTE embeddings) 
- **ğŸ¤– AI Quality Assessment**: Qwen2-based classifier with multi-source score fusion 
- **ğŸ§¹ Advanced Text Cleaning**: All text data was processed using **Ultimate Data Cleaner v7.5.0.5**, which provides robust, high-performance cleaning tailored for web-scraped and scientific data. 
- **ğŸ›¡ï¸ Contamination Prevention**: Automatic test set leak detection and removal 

## ğŸ“š Dataset Composition

### Token Distribution by Domain

| Domain | Token Count | Percentage | Description |
|--------|-------------|------------|-------------|
| **ğŸ† Nemotron CC High** | 1,468.3B | 59.7% | High quality CommonCrawl data |
| **ğŸŒ DCLM** | 314.2B | 12.8% | DCLM baseline web content |
| **ğŸ’» RefineCode** | 279.4B | 11.4% | GitHub repositories (Academic Use Only) |
| **â­ Nemotron CC Medium-High** | 254.5B | 10.3% | Medium-high quality CommonCrawl data |
| **ğŸ“š FineWeb Edu** | 117.4B | 4.8% | Educational web content |
| **ğŸŒ Chinese** | 112.18B | 4.6% | Chinese general content |
| **ğŸ§  Reasoning QA** | 86.2B | 3.5% | Instruction-following and complex reasoning tasks |
| **ğŸ”¢ Math Web** | 68.3B | 2.8% | Mathematics and scientific content |
| **ğŸ“Š MegaMath** | 28.5B | 1.2% | Specialized mathematical collections |
| **ğŸ”„ Translation** | 1.61B | 0.1% | English-Chinese translation pairs |
| **Total** | **2,460.71B** | **100%** | Complete dataset |


### ğŸ”¥ Complete Data Sources by Domain (52 Premium Datasets)

#### **ğŸ“ DCLM Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| DCLM-Baseline | `DCLM/dclm-baseline-1.0` | High-quality web content from DCLM |

#### **ğŸ“š FineWeb Edu Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| FineWeb-Edu | `HuggingFaceFW/fineweb-edu` | Educational web content (0-5 quality scale) |

#### **ğŸŒ FineWeb Edu Chinese Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| FineWeb-Edu-Chinese | `opencsg/Fineweb-Edu-Chinese-V2.1` | Chinese educational content (3.4-5.0 scale) |

#### **ğŸ”¢ Math Web Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| AutoMathText | `math-ai/AutoMathText` | Math/Code/ArXiv content with lm_q1q2_score |
| FineMath | `HuggingFaceTB/finemath` | High-quality mathematics content (0-5 scale) |
| Open-Web-Math-Pro | `gair-prox/open-web-math-pro` | Mathematical web pages |
| InfiMM-WebMath-40B | `Infi-MM/InfiMM-WebMath-40B` | Multimodal mathematical content |

#### **ğŸ† Nemotron CC High Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| Nemotron-CC (High) | `nvidia/nemotron-cc` | High-quality CommonCrawl subset |

#### **â­ Nemotron CC Medium-High Domain**Â 
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| Nemotron-CC (Medium-High) | `nvidia/nemotron-cc` | Medium-high quality CommonCrawl subset |

#### **ğŸ’» RefineCode Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| RefineCode | `m-a-p/RefineCode` | GitHub repositories (Academic Use Only) |

#### **ğŸ§  Reasoning QA Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| OPC-Annealing-Corpus | `OpenCoder-LLM/opc-annealing-corpus` | Code training corpus |
| OPC-SFT-Stage1 | `OpenCoder-LLM/opc-sft-stage1` | Instruction following data (stage 1) |
| OPC-SFT-Stage2 | `OpenCoder-LLM/opc-sft-stage2` | Instruction following data (stage 2) |
| Magpie-Reasoning-V2-250K-CoT-QwQ | `Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ` | Chain-of-thought reasoning (QwQ) |
| Magpie-Reasoning-V1-150K-CoT-QwQ | `Magpie-Align/Magpie-Reasoning-V1-150K-CoT-QwQ` | Chain-of-thought reasoning (QwQ) |
| Magpie-Reasoning-V1-150K-CoT-Deepseek-R1-Llama-70B | `Magpie-Align/Magpie-Reasoning-V1-150K-CoT-Deepseek-R1-Llama-70B` | Advanced reasoning (DeepSeek-R1) |
| Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B | `Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B` | Advanced reasoning (DeepSeek-R1) |
| General-Instruction-Augmented-Corpora | `instruction-pretrain/general-instruction-augmented-corpora` | General instruction synthesis |
| FT-Instruction-Synthesizer-Collection | `instruction-pretrain/ft-instruction-synthesizer-collection` | Fine-tuning instruction synthesis |
| Code-Feedback-Filtered-Instruction | `m-a-p/CodeFeedback-Filtered-Instruction` | Code QA with feedback |
| XCoder-80K | `banksy235/XCoder-80K` | Code instruction data |
| Orca-Math-Word-Problems-200K | `microsoft/orca-math-word-problems-200k` | Math word problems |
| Meta-Math-QA | `meta-math/MetaMathQA` | Mathematical QA dataset |
| Numina-Math-CoT | `AI-MO/NuminaMath-CoT` | Math chain-of-thought |
| Scale-Quest-Math | `dyyyyyyyy/ScaleQuest-Math` | Mathematical problem solving |
| Calc-Ape210K | `MU-NLPC/Calc-ape210k` | Chinese math problems |
| MathInstruct | `TIGER-Lab/MathInstruct` | Math instruction data |
| MathScaleQA-2M | `fdqerq22ds/MathScaleQA-2M` | Large-scale math QA |
| Gretel-Math-GSM8K-V1 | `gretelai/gretel-math-gsm8k-v1` | GSM8K style problems |
| Open-Math-Instruct-2 | `nvidia/OpenMathInstruct-2` | Open math instructions |
| Stack-Math-QA | `math-ai/StackMathQA` | Stack Exchange math QA |
| OpenR1-Math-220K | `open-r1/OpenR1-Math-220k` | Advanced math reasoning |
| Natural-Reasoning | `facebook/natural_reasoning` | Natural language reasoning |
| Math-Code-Instruct | `MathLLMs/MathCodeInstruct` | Math with code instructions |
| Math-Code-Instruct-Plus | `MathLLMs/MathCodeInstruct-Plus` | Enhanced math-code instructions |
| Open-Orca | `Open-Orca/OpenOrca` | General instruction following |
| SlimOrca-Deduped-Cleaned-Corrected | `Open-Orca/slimorca-deduped-cleaned-corrected` | Cleaned instruction data |
| Orca-AgentInstruct-1M-V1-Cleaned | `mlabonne/orca-agentinstruct-1M-v1-cleaned` | Agent instruction data |
| FOL-NLI | `tasksource/FOL-nli` | First-order logic reasoning |
| Infinity-Instruct | `BAAI/Infinity-Instruct` | Multi-domain instructions |
| Llama-Nemotron-Post-Training-Dataset-V1 | `nvidia/Llama-Nemotron-Post-Training-Dataset-v1` | Post-training dataset |
| Codeforces-CoTs | `open-r1/codeforces-cots` | Competitive programming |
| Reasoning-V1-20M | `glaiveai/reasoning-v1-20m` | Large-scale reasoning data |
| Lean-STaR-Plus | `ScalableMath/Lean-STaR-plus` | Lean formal proofs (enhanced) |
| Lean-STaR-Base | `ScalableMath/Lean-STaR-base` | Lean formal proofs (base) |
| Lean-CoT-Plus | `ScalableMath/Lean-CoT-plus` | Lean chain-of-thought (enhanced) |
| Lean-CoT-Base | `ScalableMath/Lean-CoT-base` | Lean chain-of-thought (base) |
| Lean-Github | `internlm/Lean-Github` | Lean repository code |
| Lean-Workbook | `internlm/Lean-Workbook` | Lean problem workbook |
| DeepSeek-Prover-V1 | `deepseek-ai/DeepSeek-Prover-V1` | Formal proof verification |

#### **ğŸ”„ Translation Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| UN-PC | `Helsinki-NLP/un_pc` | English-Chinese translation pairs |
| UN-PC-Reverse | `Helsinki-NLP/un_pc` | Chinese-English translation pairs |

#### **ğŸ“Š MegaMath Domain**
| Source | HuggingFace Dataset | Description |
|--------|-------------------|-------------|
| MegaMath-QA | `LLM360/MegaMath` | Large-scale mathematical QA |
| MegaMath-Translated-Code | `LLM360/MegaMath` | Mathematical code translations |
| MegaMath-Text-Code-Block | `LLM360/MegaMath` | Mixed math text and code blocks |

**Total: 52 Premium Data Sources** with official HuggingFace dataset links covering web content, mathematics, code, reasoning, formal proofs, and bilingual data.

## ğŸ› ï¸ Processing Pipeline

### 1. **Data Extraction & Standardization**
```python
{
Â  Â  "domain_prefix": "lbty.org",
Â  Â  "id": "117b6a7d-5126-41fe-9bc2-d276e98632e6",
Â  Â  "meta": "{\"domain\": \"dclm\", \"ori_score\": 0.043276190757751465, \"source\": \"dclm_baseline\"}",
Â  Â  "text": "Sabine Expedition\n\nThe Sabine Expedition was an expedition approved by the United States Congress in 1806...",
Â  Â  "tokens": 145,Â  # Token count using Qwen2.5 tokenizer
Â  Â  "url": "[https://lbty.org/american-indian-battles/sabine-expedition/](https://lbty.org/american-indian-battles/sabine-expedition/)",
Â  Â  "score": 0.19072403013706207
}
````

### 2\. **Three-Tier Deduplication**

#### ğŸ¯ **Exact Deduplication**

  - SHA256 content hashing
  - Priority-based duplicate resolution
  - **Result**: \~30% exact duplicates removed

#### ğŸ”„ **Fuzzy Deduplication**Â 

  - MinHash Locality Sensitive Hashing (LSH)
  - Jaccard similarity threshold: 0.9
  - Connected components clustering
  - **Result**: \~20% near-duplicates removed

#### ğŸ§  **Semantic Deduplication**

  - `Alibaba-NLP/gte-multilingual-base` embeddings
  - K-means clustering (k=100,000)Â Â 
  - Cosine similarity threshold: 0.007
  - **Result**: \~10% semantic duplicates removed

### 3\. **ğŸ¤– AI Quality Assessment**

**Qwen2-Based Classifier Architecture**:

  - Fine-tuned regression head for quality scoring
  - Multi-source score normalization and fusion
  - MSE loss with sigmoid activation

### 4\. **ğŸ§¹ Advanced Text Cleaning**

All text data was processed using **Ultimate Data Cleaner v7.5.0.5**, which provides robust, high-performance cleaning tailored for web-scraped and scientific data.

**Key Features Used:**

  - **Advanced LaTeX & Code Protection**: protect complex nested LaTeX environments (`\begin{}...\end{}`), inline math (`$...$`), commands, and markdown code fences. 
  - **Quality Heuristics**: Removes corrupted samples with excessive repetition, severe bracket imbalances, etc. 

### 5\. **ğŸ›¡ï¸ Contamination Detection**

**Test Set Protection**:

  - Math dataset test questions
  - GSM8K evaluation problemsÂ Â 
  - Exact string matching with preprocessing
  - Automatic filtering during data extraction

## ğŸš€ How to Use

### Loading with Datasets

```python
from datasets import load_dataset

# Load full dataset
dataset = load_dataset("OpenSQZ/AutoMathText-V2", streaming=True)

# Load specific domain
math_data = load_dataset("OpenSQZ/AutoMathText-V2", name="math_web", streaming=True)
```

### ğŸ’» RefineCode Content Download

**Important**: For the RefineCode domain, only metadata is included in the dataset. The actual code content was removed to reduce storage requirements. To access the full code content, use the `blob_id` field from the metadata to download from AWS S3:

```python
import os
import json
import boto3
from smart_open import open
from datasets import load_dataset

# Setup AWS credentials
session = boto3.Session(
Â  Â  aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
Â  Â  aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"]
)
s3 = session.client("s3")

def download_code_content(blob_id, src_encoding):
Â  Â  """Download code content from AWS S3 using blob_id"""
Â  Â  s3_url = f"s3://softwareheritage/content/{blob_id}"
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  with open(s3_url, "rb", compression=".gz", transport_params={"client": s3}) as fin:
Â  Â  Â  Â  Â  Â  content = fin.read().decode(src_encoding)
Â  Â  Â  Â  return {"content": content}
Â  Â  except Exception as e:
Â  Â  Â  Â  return {"content": None, "error": str(e)}

# Load RefineCode domain
refinecode_data = load_dataset("OpenSQZ/AutoMathText-V2", name="refinecode", streaming=True)

# Process each sample to download content
for sample in refinecode_data:
Â  Â  # Parse metadata to extract blob_id and encoding
Â  Â  meta = json.loads(sample["meta"])
Â  Â  blob_id = meta.get("blob_id")
Â  Â  src_encoding = meta.get("src_encoding", "utf-8")
Â  Â Â 
Â  Â  if blob_id:
Â  Â  Â  Â  # Download the actual code content
Â  Â  Â  Â  code_data = download_code_content(blob_id, src_encoding)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Combine metadata with downloaded content
Â  Â  Â  Â  full_sample = {
Â  Â  Â  Â  Â  Â  **sample,
Â  Â  Â  Â  Â  Â  "code_content": code_data["content"]
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  print(f"Downloaded content for {sample['id']}")
Â  Â  Â  Â  print(f"Content length: {len(code_data['content']) if code_data['content'] else 0}")
Â  Â  Â  Â  break
```

**Requirements**:

  - AWS credentials with access to Software Heritage S3 bucket
  - `smart_open` library: `pip install smart_open[s3]`
  - `boto3` library: `pip install boto3`

**Note**: This download method is required only for the RefineCode domain. All other domains contain the full text content directly in the dataset.

## ğŸŒ Dataset Structure & Configurations

### Directory Structure

The dataset is organized by domain with quality-based token splits:

```
AutoMathText-V2/
â”œâ”€â”€ dclm/Â  Â  Â  Â  Â  Â  Â  Â  Â  # DCLM baseline web content
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens (score-based)
â”‚Â  Â â”œâ”€â”€ 10-20/Â  Â  Â  Â  Â  Â  # 10-20% quality tokens
â”‚Â  Â â”œâ”€â”€ 20-30/Â  Â  Â  Â  Â  Â  # 20-30% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ fineweb_edu/Â  Â  Â  Â  Â  Â # FineWeb educational content
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ 10-20/Â  Â  Â  Â  Â  Â  # 10-20% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ fineweb_edu_chinese/Â  Â # Chinese educational content
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ math_web/Â  Â  Â  Â  Â  Â  Â  # Mathematics and scientific content
â”‚Â  Â â”œâ”€â”€ 0-10/Â  . Â  Â  Â  Â  Â # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ megamath/Â  Â  Â  Â  Â  Â  Â  # Specialized math collections
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ nemotron_cc_high/Â  Â  Â  # High quality Nemotron CommonCrawl
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ nemotron_cc_medium_high/ # Medium-high quality Nemotron CommonCrawl
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  . # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ reasoning_qa/Â  Â  Â  Â  Â  # Instruction and reasoning data
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â”œâ”€â”€ refinecode/Â  Â  Â  Â  Â  Â  # GitHub code repositories (Academic Use Only)
â”‚Â  Â â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens
â”‚Â  Â â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
â”‚Â  Â â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
â””â”€â”€ translation/Â  Â  Â  Â  Â  Â # English-Chinese translation pairs
Â  Â  â”œâ”€â”€ 0-10/Â  Â  Â  Â  Â  Â  Â # Bottom 10% quality tokens
Â  Â  â”œâ”€â”€ ...Â  Â  Â  Â  Â  Â  Â  Â # Additional percentile ranges
Â  Â  â””â”€â”€ 90-100/Â  Â  Â  Â  Â  Â # Top 10% highest quality tokens
```

### Quality-Based Token Distribution

Each domain is divided into **10 quality percentiles** (0-10, 10-20, ..., 90-100) based on:

  - **Token count**: Equal number of tokens per percentile bucket
  - **Quality scores**: AI classifier scores from Qwen2-based quality assessment
  - **Percentile ranking**: Higher percentiles contain higher quality content

### Available Configurations

  - **Domain-specific configs**: Load individual domains (`dclm`, `fineweb_edu`, `math_web`, `reasoning_qa`, etc.)
  - **Quality-filtered configs**: Load specific quality ranges (e.g., `dclm/90-100` for top quality DCLM content)
  - **Nemotron variants**: Choose between `nemotron_cc_high` and `nemotron_cc_medium_high` based on quality needs
  - **Combined configs**: Mix domains and quality levels based on training requirements
  - **Custom sampling**: Select percentile ranges across multiple domains for balanced training

### Language Distribution

  - **English**: \~95% of content
  - **Chinese**: \~5% of content

## ğŸ”¬ Technical Deep Dive

For detailed technical documentation, including:

  - Complete processing pipeline specificationsÂ Â 
  - Deduplication algorithm details
  - Quality classifier training procedures
  - Contamination detection methodology

Please refer to our [Technical Documentation](https://iiis-ai.github.io/AutoMathText-V2) and [GitHub Repository](https://github.com/iiis-ai/AutoMathText-V2).

## ğŸ¤ Contributing

We welcome contributions to improve dataset quality and processing techniques:

  - ğŸ› **Bug Reports**: Issues with data quality or processing
  - ğŸ’¡ **Feature Requests**: New data sources or processing improvementsÂ Â 
  - ğŸ“š **Documentation**: Help improve our guides and examples
  - ğŸ”¬ **Research**: Collaborate on quality assessment and deduplication methods

## ğŸ“œ Licensing & Citation

### License

Released under **AutoMathText Data Agreement for Model Training** (See [LICENSE](https://github.com/iiis-ai/AutoMathText-V2/blob/master/LICENSE)).Â 

### Citation

```bibtex
@misc{automathtext_v2_2025,
Â  title={AutoMathText-V2: A 2.46 Trillion Token AI-Curated STEM Pretraining Dataset},
Â  author={Li, Chao and Zhang, Yifan and Yuan, Yang and Yao, Andrew C},
Â  year={2025},
Â  publisher={Hugging Face},
Â  url={https://huggingface.co/datasets/OpenSQZ/AutoMathText-V2},
Â  note={A 2.46T token multi-domain dataset with fine-grained deduplication and AI-powered quality assessment.}
}

@article{zhang2025autonomous,
Â  title={Autonomous Data Selection with Zero-shot Generative Classifiers for Mathematical Texts},
Â  author={Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C},
Â  journal={The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025 Findings)},
Â  year={2025}
}
```
